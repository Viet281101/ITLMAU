# TP 2 : nombre moyen de mots par phrase et profilage morphosyntaxique

**Objectif**
On souhaite r√©cup√©rer pour un.e auteur.ice le nombre moyen de mots par phrase, puis repr√©senter un document par la r√©partition des cat√©gories grammaticales des mots qui le composent.

**Pourquoi ?** On a parfois besoin de d√©velopper ou d'adapter depuis l'existant de nouveaux outils de segmentation pour une t√¢che sp√©cifique ou un corpus sp√©cifique (texte issu de twitter, de conversations, po√©sie du XVII√® si√®cle, oral transcrit, etc.).

**Fichiers de test**
En fran√ßais, commencez par tester votre programme sur le fichier `conte.txt`. Des "tests" additionnels sont renseign√©s en fin d'√©nonc√©. Si vous pensez √† d'autres tests, partagez-les.
Attention, si vous d√©cidez de ne pas faire l'exercice en fran√ßais, vous devez utiliser des textes de test pour votre programme qui inclut tout de m√™me des guillemets, diff√©rentes ponctuations, etc. afin de pouvoir illustrer le fonctionnement de votre programme.

	Attendus
	Dans un fichier README, vous pr√©sentez les diff√©rentes m√©thodes test√©es, 
	donnez vos commentaires sur les output obtenus, 
	et indiquez le temps d'ex√©cution de chacune des m√©thodes 
	(par exemple en utilisant la commande bash `time`)

	Produisez du code modulaire autant que possible : 
	vous allez devoir √† plusieurs reprises ouvrir, 
	√©crire et fermer des fichiers, 
	utilisez a minima des fonctions pour ne pas rendre de code redondant.


:question: Quelles sont les √©tapes principales √† r√©aliser pour atteindre l'objectif ?

> Details
> [1. d√©couper le texte en phrases](#1-d√©coupage-en-phrases--test-de-diff√©rentes-m√©thodes)
> [2. d√©couper le texte en mots](#2-d√©coupage-en-mots--test-de-diff√©rentes-m√©thodes)
> [3. annoter en cat√©gories grammaticales les mots du texte](#3-annotation-morphosyntaxique)
> [4. choisir un format de repr√©sentation](#4-utilisation)



## 1. D√©coupage en phrases : test de diff√©rentes m√©thodes

Dans cette section vous devez dans les 3 cas produire un programme qui d√©coupe en phrases le texte d'un fichier `input.txt` et cr√©e un fichier xml `output.xml` d√©coup√© en phrases de la forme :
```xml
<?xml version="1.0" encoding="utf-8"?>
<text>
<sent id='0'>Le caillou qui voulait voir le monde - Conte pour enfant
</sent>
<sent id='1'>Par Sabine D'Halluin
</sent>
<sent id='2'>C‚Äôest l‚Äôhistoire d‚Äôun petit caillou au bord d‚Äôun chemin.</sent>
<sent id='3'> Un chemin de campagne.</sent>
```

### 1.1 La m√©thode "√† base de r√®gles"

Pour cet exercice, seules les expressions r√©guli√®res sont autoris√©es (pas de librairie d√©di√©e au d√©coupage), vous pouvez utiliser le langage de votre choix.

Pour python :

- [module re](https://docs.python.org/3/howto/regex.html)
- [howto regex](https://docs.python.org/3/howto/regex.html)
- [I/O (read et write)](https://docs.python.org/3/tutorial/inputoutput.html)


### 1.2 Utilisation de librairies d√©di√©es (python)

1. `Sentencizer`

Il existe un certain nombre de librairies permettant de faire des traitements sur la langue. Attention, toutes les librairies n'existent pas pour toutes les langues ni avec les m√™mes performances.

Utilisez le composant "sentencizer" de la m√©thode `add_pipe` du module `French` de `spacy.lang.fr` pour proposer une autre solution.
[composant Sentencizer Spacy](https://spacy.io/api/sentencizer)

2. `Senter`
	Rappel pipeline spacy :
![alt text](image.png)
source : https://spacy.io/api

[Documentation pipeline nlp](https://spacy.io/usage/processing-pipelines) : la m√©thode nlp fait l'ensemble des traitements ci-dessus gr√¢ce au composants list√©s ici : [composants](https://spacy.io/usage/processing-pipelines#built-in), et cr√©e un objet **Doc** comme illustr√© ci-dessus.
Pour acc√©der aux annotations produites par chaque module, consultez la documentation correspondante : par exemple, les entit√©s sont accessibles via **Doc.ents** (voir [documentation entity recognizer (ner)](https://spacy.io/api/entityrecognizer)).

__:warning: Ce n'est pas parce qu'une librairie existe 
et est joliment pr√©sent√©e qu'elle produit un r√©sultat syst√©matiquement correct.
ex : [performances annonc√©es de chaque module (fran√ßais)](https://spacy.io/models/fr#fr_core_news_sm-accuracy)__

Utilisez le 'senter' d'un mod√®le de langue tel que fr_core_news_sm pour proposer une troisi√®me solution : [Senter Spacy](https://spacy.io/api/sentencerecognizer).

## 2. D√©coupage en mots : test de diff√©rentes m√©thodes

Le d√©coupage en mots est une t√¢che facile en apparence et dont la difficult√© varie largement selon les langues.
Les `choix` faits lors de cette √©tape vont avoir une influence √† l'√©tape suivante, c'est-√†-dire sur une "`downstream task`".

Si le segment `c'est-√†-dire` n'est pas d√©coup√©, alors il est consid√©r√© comme un adverbe : `c'est-√†-dire/ADV`
S'il est d√©coup√©, on peut se retrouver avec `c'/PRON` `-/PUNCT` `est/VERB` `-/PUNCT` `√†/ADP` `-/PUNCT` `dire/VERB`.

Les choix qui sont fait peuvent √† peu pr√®s toujours √™tre justifi√©s. L'important est de les documenter, pour savoir √† quoi s'attendre quand on utilise un outil. En premi√®re approximation, on peut se dire que si un mot existe dans le dictionnaire en tant que tel (par exemple [c'est-√†-dire](https://www.larousse.fr/dictionnaires/francais/c_est-%C3%A0-dire/14350), alors on consid√®re que c'est un `token` √† part enti√®re.

### 2.1 Tok√©niseur "√† base de r√®gles"

Commencez par observer le r√©sultat du d√©coupage "selon les espaces". Am√©liorez ce r√©sultat en incluant √† votre tok√©niseur les ponctuations et tout ce qui vous para√Æt n√©cessaire pour tok√©niser correctement le texte suivant :


	Depuis huit jours, j‚Äôavais d√©chir√© mes bottines
	Aux cailloux des chemins. J‚Äôentrais √† Charleroi.
	‚Äì Au Cabaret-Vert : je demandai des tartines
	De beurre et du jambon qui f√ªt √† moiti√© froid.
[Au Cabaret-Vert, cinq heures du soir - Arthur Rimbaud](https://fr.wikisource.org/wiki/Po%C3%A9sies_(Rimbaud)/%C3%A9d._Vanier,_1895/Au_Cabaret-Vert,_cinq_heures_du_soir)



### 2.2 Librairie NLTK (Natural Language ToolKit)

Dans un premier temps, t√©l√©chargez le module 'punkt' de la librairie nltk :

```bash
$ python
>>> import nltk
>>> nltk.download('punkt') 
```

puis testez le code suivant :

```python
import nltk

from nltk.tag import StanfordPOSTagger
from textblob import TextBlob

# Le chemin est √† sp√©cifier selon la localisation du
# module punkt
tokenizer = nltk.data.load('##TO CHANGE##/punkt/french.pickle')
words_generator = tokenizer._tokenize_words("Depuis huit jours, j'avais d√©chir√© mes bottines Aux cailloux des chemins. J'entrais √† Charleroi. - Au Cabaret-Vert : je demandai des tartines De beurre et du jambon qui f√ªt √† moiti√© froid.")
words = [word for word in words_generator]
```

Que pensez-vous du r√©sultat ?


### 2.2 Tok√©niseur SpaCy

Le fonctionnement du tok√©niseur de la pipeline Spacy est expliqu√© ici : [documentation](https://spacy.io/usage/linguistic-features#tokenization)
Utilisez la documentation de spacy pour tok√©niser votre texte d'une seconde mani√®re [documentation](https://spacy.io/usage/spacy-101).

Pr√™tez attention √† ce que vous avez gagn√©/perdu par rapport √† la m√©thode pr√©c√©dente.

### 2.3 Remarque et tok√©niseur custom

Il parfois utile de combiner plusieurs outils en cascade en fonction de vos besoins. Par exemple, testez d'ajouter un "#IDL" dans une cha√Æne de texte √† tok√©niser : il y a de fortes chances que vous vous retrouviez avec le caract√®re `#` s√©par√© de `IDL`. Pour une application o√π vous souhaitez consid√©rer que les hashtags doivent √™tre regroup√©s, vous pouvez customiser votre tok√©niseur ou faire une seconde passe qui regroupe le signe `#` et le token qui suit.

Les tok√©niseurs ci-dessus ne sont pas compl√®tement satisfaisants. Vous aurez besoin d'un outil pour votre projet, donc gardez en t√™te les qualit√©s et les d√©fauts √©ventuels des outils existants, quitte √† am√©liorer leur sortie pour vos besoins.

Utilisez l'outil (ou la combinaison d'outils) de votre choix et produisez √† partir du document `conte.txt` un nouveau fichier xml o√π chaque phrase est entour√©e des balises `<sentence></sentence>` et chaque mot est entour√© des balises `<mot></mot>`.

## 3. Annotation morphosyntaxique

Si vous avez bien lu la documentation de SpaCy, vous avez d√ª voir que la pipeline `nlp` produit aussi les annotations morphosyntaxiques accessibles via `token.pos_`. Ajoutez cette information sous la forme qui vous semble le plus pratique (nouvel √©l√©ment ? attribut ?) dans le fichier xml contenant les phrases et les mots tok√©nis√©s.

Quelles sont les performances annonc√©es par SpaCy pour ce module ? D√©tectez-vous des erreurs ?


## 4. Utilisation

1. Utilisez vos programmes pour comparer le nombre moyen de mots par phrases chez deux auteurs/trices de votre choix.

2. Proposez une visualisation de vos deux corpus (cela peut √™tre simplement un texte de chaque auteur/trice) selon par exemple leur proportion respective de chaque cat√©gorie grammaticale, les mots les plus fr√©quents pour chaque auteur/trice, etc.
Vous pouvez utiliser XSL pour mettre en forme votre XML ou bien les librairies de votre choix pour produire un affichage permettant de faire une comparaison.


### Tests de vos programmes

- Sur https://pablockchain.fr retrouvez mes conf√©rences (captation üé¨ & dates üìÜ), billets de blog üìù et articles scientifiques üì∞ au sujet des #blockchains #cryptomonnaies #NFT #web3‚Ä¶ üìÆVous pouvez m'y laisser votre mail pour √™tre inform√©¬∑e de mes futures interventions üòä." ([ici](https://twitter.com/p4bl0/status/1708175577251786954))
- M. Revault d'Allonnes est joignable par mail √† l'adresse : ara@up8.edu ou au +33 0149406497.
- testez sur un tr√®s long texte (par exemple g√©n√©r√© par [ipsum.one](https://ipsum.one/fr/paragraphes/50))


## Installations

  - installation package Spacy :


```
apt update
apt install python3-pip
pip install spacy
```


 - (√©ventuellement) installation de PyCharm ou d'un √©diteur qui vous permet d'acc√©der facilement √† la `documentation` des diff√©rentes fonctions.
	t√©l√©chargement du "mod√®le de langue" du fran√ßais

```
python3 -m spacy download fr_core_news_sm
```